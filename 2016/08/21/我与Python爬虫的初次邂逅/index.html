<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="YoungSx"><title>我与Python爬虫的初次邂逅 · 不懂鸟语的人</title><meta name="description" content="背景自己一直喊着要学爬虫，但是总是因为各种各样的事情耽误了。最近感觉不能再颓废了，于是乎重新拾起来这个小小的目标，开始学习。
开始先是在知乎上如何入门 Python 爬虫？这个问题下看了看爬虫的基本概念和原理，发现原理和我之前的理解并没有什么出入，只是集群那一块是我之前没有关注到的。
然后，我找到了"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">不懂鸟语的人</a></h3><div class="description"><p>YoungSx 的个人学习笔记。精力原因，也许只为自己方便翻阅。</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/YangShangxin"><i class="fa fa-twitter"></i></a></li><li><a href="http://github.com/YoungSx"><i class="fa fa-github"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>我与Python爬虫的初次邂逅</a></h3></div><div class="post-content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>自己一直喊着要学爬虫，但是总是因为各种各样的事情耽误了。最近感觉不能再颓废了，于是乎重新拾起来这个小小的目标，开始学习。</p>
<h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>先是在知乎上<a href="https://www.zhihu.com/question/20899988" target="_blank" rel="noopener">如何入门 Python 爬虫？</a>这个问题下看了看爬虫的基本概念和原理，发现原理和我之前的理解并没有什么出入，只是集群那一块是我之前没有关注到的。</p>
<p>然后，我找到了一个技术博客，看着那位程序媛写的一系列教程，基于Python3开始了我爬取单页面图片的尝试。</p>
<p>HTTP库用的是Urllib，HTML的解析工具，选用的是BeautifulSoup这个库。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="comment">#用做解析</span></span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="comment">#文件读写</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回请求到的内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">url_open</span><span class="params">(url)</span>:</span></span><br><span class="line">    req = urllib.request.Request(url)</span><br><span class="line"></span><br><span class="line">    response = urllib.request.urlopen(req)</span><br><span class="line">    html = response.read()</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_links</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># 获取HTML源码</span></span><br><span class="line">    html = url_open(url)</span><br><span class="line">    <span class="comment">#解析HTML</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">    <span class="comment">#定位父div</span></span><br><span class="line">    divs = soup(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'pic'</span>&#125;)</span><br><span class="line">    <span class="comment">#存储img的列表</span></span><br><span class="line">    img_addrs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">#拿到每一个div中的img及其的src</span></span><br><span class="line">    <span class="keyword">for</span> div <span class="keyword">in</span> divs:</span><br><span class="line">        img_urls = div.find(<span class="string">'img'</span>)[<span class="string">'src'</span>]</span><br><span class="line">        img_addrs.append(img_urls)</span><br><span class="line">    <span class="keyword">return</span> img_addrs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_imgs</span><span class="params">(folder,img_addrs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> img_addrs:</span><br><span class="line">        filename = each.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment">#读取网络图片</span></span><br><span class="line">            img = url_open(each)</span><br><span class="line">            f.write(img)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_pic</span><span class="params">(folder = <span class="string">'pic'</span>)</span>:</span></span><br><span class="line">    url = <span class="string">'http://jcss.shangxin.link/spiderTest.html'</span></span><br><span class="line">    img_addrs = get_links(url)</span><br><span class="line"></span><br><span class="line">    os.mkdir(folder)</span><br><span class="line">    os.chdir(folder)</span><br><span class="line"></span><br><span class="line">    save_imgs(<span class="string">'pic'</span>, img_addrs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    download_pic()</span><br></pre></td></tr></table></figure>
<p>这段代码的功能只是在一个单页面上爬取一些照片，由于初学，自己挂了个结构简单的测试页面以供爬虫爬取测试…</p>
<h2 id="踩的坑"><a href="#踩的坑" class="headerlink" title="踩的坑"></a>踩的坑</h2><ul>
<li>在这里，我特别注意到了网上很多代码中用的是Urllib2这个库，到了Python3中无法运行，这是因为Urllib和Urllib2出现在python2中，在3中，统一整合Urllib ，而没有了Urllib2。</li>
<li>urllib.request.urlopen()的参数中输入的是一个页面地址，有一次忘记加上协议（<a href="http://），导致报错（这不算坑，逃" target="_blank" rel="noopener">http://），导致报错（这不算坑，逃</a>…</li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2016-08-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/Python/" title="Python">Python </a><a class="tag" href="/tags/爬虫/" title="爬虫">爬虫 </a><a class="tag" href="/tags/BeautifulSoup4/" title="BeautifulSoup4">BeautifulSoup4 </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2016/08/21/我与Python爬虫的初次邂逅/,不懂鸟语的人,我与Python爬虫的初次邂逅,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2017/01/16/美丽有罪？/" title="美丽有罪？">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2016/07/31/SS-Panel-SS-Manyuser安装部署/" title="SS-Panel+SS-Manyuser安装部署">下一篇</a></li></ul></div><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@v1.1.7/dist/Valine.min.js?v=undefined"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'b47PMHdzebB0ozGGdW68aQAN-gzGzoHsz',
  app_key:'pnCuygqUh5uDjQt8wNEKSd8K',
  placeholder:'Just go go',
  path: window.location.pathname,
  avatar:'mm'
})</script></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>